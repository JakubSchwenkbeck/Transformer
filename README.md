# Transfomer
writing a Transformer (base for LLMs) in Rust





## References

- **Ashish Vaswani et al.** (2017). *Attention Is All You Need*. [Read the Paper (PDF)](https://arxiv.org/pdf/1706.03762)
- **Alec Radford et al.** *Language Models are Unsupervised Multitask Learners*. [Read the Paper (PDF)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- **Harvard NLP Group.** *The Annotated Transformer*. [Read the Article](https://nlp.seas.harvard.edu/annotated-transformer/)
- **3Blue1Brown** *Deep-Learning Series*. [Introduction to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M) , [Attention in Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)
---

## **License**
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
